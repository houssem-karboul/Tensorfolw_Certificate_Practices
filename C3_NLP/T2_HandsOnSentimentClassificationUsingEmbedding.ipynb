{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Training a binary classifier with the Sarcasm Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aefe07441e55e1f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "344fbf6e117fc03d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://storage.googleapis.com/tensorflow-1-public/course3/sarcasm.json\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"sarcasm.json\", \"wb\") as file:\n",
    "    file.write(response.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:03.613925Z",
     "start_time": "2024-04-03T16:26:00.124598Z"
    }
   },
   "id": "991b5f3cad759568",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"./sarcasm.json\", 'r') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "# Initialize the lists\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "# Collect sentences and labels into the lists\n",
    "for item in datastore:\n",
    "    sentences.append(item['headline'])\n",
    "    labels.append(item['is_sarcastic'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:03.674894Z",
     "start_time": "2024-04-03T16:26:03.615923Z"
    }
   },
   "id": "15aaa31b041deeae",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Number of examples to use for training\n",
    "training_size = 20000\n",
    "\n",
    "# Vocabulary size of the tokenizer\n",
    "vocab_size = 10000\n",
    "\n",
    "# Maximum length of the padded sequences\n",
    "max_length = 32\n",
    "\n",
    "# Output dimensions of the Embedding layer\n",
    "embedding_dim = 16"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:03.690415Z",
     "start_time": "2024-04-03T16:26:03.675895Z"
    }
   },
   "id": "973638a375922542",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Split the sentences\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "\n",
    "# Split the labels\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:03.706440Z",
     "start_time": "2024-04-03T16:26:03.691420Z"
    }
   },
   "id": "de8d886111a63712",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing the train and test sets\n",
    "Tokenization  => Padding "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4d1150177fdeb06"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Parameters for padding and OOV tokens\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "# Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Generate pad and the training sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Generate and pad the testing sequences\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# Convert the labels lists into numpy arrays\n",
    "training_labels = np.array(training_labels)\n",
    "testing_labels = np.array(testing_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:06.767716Z",
     "start_time": "2024-04-03T16:26:03.708440Z"
    }
   },
   "id": "97e27679b6f5d3b",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build and Compile the Model\n",
    " =>GlobalAveragePooling1D layer instead of Flatten after the Embedding. 🧑‍💻☝️\n",
    " =>it gets the average over 3 arrays (i.e. (10 + 1 + 1) / 3 and (2 + 3 + 1) / 3 to arrive at the final output.\n",
    " =>This added computation reduces the dimensionality of the model as compared to using Flatten() and thus, the number of training parameters will also decrease. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12d9f85be8f6b785"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sample_array = (1, 3, 2)\n",
      "sample array: [[[10  2]\n",
      "  [ 1  3]\n",
      "  [ 1  1]]]\n",
      "output shape of gap1d_layer: (1, 2)\n",
      "output array of gap1d_layer: [[4 2]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Initialize a GlobalAveragePooling1D (GAP1D) layer\n",
    "gap1d_layer = tf.keras.layers.GlobalAveragePooling1D()\n",
    "\n",
    "# Define sample array\n",
    "sample_array = np.array([[[10,2],[1,3],[1,1]]])\n",
    "\n",
    "# Print shape and contents of sample array\n",
    "print(f'shape of sample_array = {sample_array.shape}')\n",
    "print(f'sample array: {sample_array}')\n",
    "\n",
    "# Pass the sample array to the GAP1D layer\n",
    "output = gap1d_layer(sample_array)\n",
    "\n",
    "# Print shape and contents of the GAP1D output array\n",
    "print(f'output shape of gap1d_layer: {output.shape}')\n",
    "print(f'output array of gap1d_layer: {output.numpy()}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:06.921353Z",
     "start_time": "2024-04-03T16:26:06.768927Z"
    }
   },
   "id": "ff02e5c491f8e2cc",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91bfdf77f24e0d42"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 16)            160000    \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 16)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                408       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160433 (626.69 KB)\n",
      "Trainable params: 160433 (626.69 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:07.044895Z",
     "start_time": "2024-04-03T16:26:06.922220Z"
    }
   },
   "id": "4e90371955d29b8a",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compile the model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d060f95dcd2900f6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:07.060551Z",
     "start_time": "2024-04-03T16:26:07.046378Z"
    }
   },
   "id": "a76ac7c97a66969b",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the Model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0109b81a7a89dfd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "625/625 - 2s - loss: 0.5661 - accuracy: 0.6976 - val_loss: 0.3974 - val_accuracy: 0.8313 - 2s/epoch - 4ms/step\n",
      "Epoch 2/30\n",
      "625/625 - 1s - loss: 0.3129 - accuracy: 0.8740 - val_loss: 0.3429 - val_accuracy: 0.8536 - 921ms/epoch - 1ms/step\n",
      "Epoch 3/30\n",
      "625/625 - 2s - loss: 0.2359 - accuracy: 0.9082 - val_loss: 0.3424 - val_accuracy: 0.8521 - 2s/epoch - 3ms/step\n",
      "Epoch 4/30\n",
      "625/625 - 2s - loss: 0.1911 - accuracy: 0.9267 - val_loss: 0.3607 - val_accuracy: 0.8498 - 2s/epoch - 3ms/step\n",
      "Epoch 5/30\n",
      "625/625 - 3s - loss: 0.1588 - accuracy: 0.9401 - val_loss: 0.3848 - val_accuracy: 0.8490 - 3s/epoch - 5ms/step\n",
      "Epoch 6/30\n",
      "625/625 - 2s - loss: 0.1343 - accuracy: 0.9516 - val_loss: 0.4155 - val_accuracy: 0.8471 - 2s/epoch - 4ms/step\n",
      "Epoch 7/30\n",
      "625/625 - 2s - loss: 0.1151 - accuracy: 0.9599 - val_loss: 0.4532 - val_accuracy: 0.8456 - 2s/epoch - 2ms/step\n",
      "Epoch 8/30\n",
      "625/625 - 1s - loss: 0.0996 - accuracy: 0.9658 - val_loss: 0.4921 - val_accuracy: 0.8396 - 930ms/epoch - 1ms/step\n",
      "Epoch 9/30\n",
      "625/625 - 1s - loss: 0.0854 - accuracy: 0.9733 - val_loss: 0.5352 - val_accuracy: 0.8371 - 933ms/epoch - 1ms/step\n",
      "Epoch 10/30\n",
      "625/625 - 1s - loss: 0.0746 - accuracy: 0.9765 - val_loss: 0.5845 - val_accuracy: 0.8331 - 783ms/epoch - 1ms/step\n",
      "Epoch 11/30\n",
      "625/625 - 1s - loss: 0.0648 - accuracy: 0.9794 - val_loss: 0.6469 - val_accuracy: 0.8293 - 841ms/epoch - 1ms/step\n",
      "Epoch 12/30\n",
      "625/625 - 1s - loss: 0.0580 - accuracy: 0.9819 - val_loss: 0.6855 - val_accuracy: 0.8262 - 871ms/epoch - 1ms/step\n",
      "Epoch 13/30\n",
      "625/625 - 1s - loss: 0.0505 - accuracy: 0.9843 - val_loss: 0.7371 - val_accuracy: 0.8208 - 839ms/epoch - 1ms/step\n",
      "Epoch 14/30\n",
      "625/625 - 1s - loss: 0.0439 - accuracy: 0.9873 - val_loss: 0.7983 - val_accuracy: 0.8179 - 817ms/epoch - 1ms/step\n",
      "Epoch 15/30\n",
      "625/625 - 1s - loss: 0.0377 - accuracy: 0.9894 - val_loss: 0.8581 - val_accuracy: 0.8161 - 806ms/epoch - 1ms/step\n",
      "Epoch 16/30\n",
      "625/625 - 1s - loss: 0.0335 - accuracy: 0.9905 - val_loss: 0.9178 - val_accuracy: 0.8155 - 824ms/epoch - 1ms/step\n",
      "Epoch 17/30\n",
      "625/625 - 1s - loss: 0.0306 - accuracy: 0.9907 - val_loss: 0.9814 - val_accuracy: 0.8147 - 934ms/epoch - 1ms/step\n",
      "Epoch 18/30\n",
      "625/625 - 1s - loss: 0.0260 - accuracy: 0.9924 - val_loss: 1.0327 - val_accuracy: 0.8119 - 758ms/epoch - 1ms/step\n",
      "Epoch 19/30\n",
      "625/625 - 1s - loss: 0.0232 - accuracy: 0.9938 - val_loss: 1.1144 - val_accuracy: 0.8098 - 757ms/epoch - 1ms/step\n",
      "Epoch 20/30\n",
      "625/625 - 1s - loss: 0.0213 - accuracy: 0.9940 - val_loss: 1.1782 - val_accuracy: 0.8094 - 798ms/epoch - 1ms/step\n",
      "Epoch 21/30\n",
      "625/625 - 1s - loss: 0.0183 - accuracy: 0.9946 - val_loss: 1.2457 - val_accuracy: 0.8076 - 784ms/epoch - 1ms/step\n",
      "Epoch 22/30\n",
      "625/625 - 1s - loss: 0.0167 - accuracy: 0.9958 - val_loss: 1.2835 - val_accuracy: 0.8071 - 804ms/epoch - 1ms/step\n",
      "Epoch 23/30\n",
      "625/625 - 1s - loss: 0.0148 - accuracy: 0.9961 - val_loss: 1.3599 - val_accuracy: 0.8065 - 715ms/epoch - 1ms/step\n",
      "Epoch 24/30\n",
      "625/625 - 1s - loss: 0.0133 - accuracy: 0.9963 - val_loss: 1.4389 - val_accuracy: 0.8062 - 690ms/epoch - 1ms/step\n",
      "Epoch 25/30\n",
      "625/625 - 1s - loss: 0.0125 - accuracy: 0.9965 - val_loss: 1.5340 - val_accuracy: 0.8047 - 780ms/epoch - 1ms/step\n",
      "Epoch 26/30\n",
      "625/625 - 1s - loss: 0.0110 - accuracy: 0.9967 - val_loss: 1.5249 - val_accuracy: 0.8052 - 853ms/epoch - 1ms/step\n",
      "Epoch 27/30\n",
      "625/625 - 1s - loss: 0.0110 - accuracy: 0.9969 - val_loss: 1.5784 - val_accuracy: 0.8015 - 1s/epoch - 2ms/step\n",
      "Epoch 28/30\n",
      "625/625 - 1s - loss: 0.0090 - accuracy: 0.9976 - val_loss: 1.6397 - val_accuracy: 0.8035 - 1s/epoch - 2ms/step\n",
      "Epoch 29/30\n",
      "625/625 - 2s - loss: 0.0091 - accuracy: 0.9973 - val_loss: 1.7052 - val_accuracy: 0.8034 - 2s/epoch - 3ms/step\n",
      "Epoch 30/30\n",
      "625/625 - 2s - loss: 0.0090 - accuracy: 0.9973 - val_loss: 1.9164 - val_accuracy: 0.8001 - 2s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:42.876180Z",
     "start_time": "2024-04-03T16:26:07.062292Z"
    }
   },
   "id": "11113cc9537bb5e4",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize the Results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot utility\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "# Plot the accuracy and loss\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "905cbf278268777"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2decadad85630e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Visualize the final weights of the embeddings\n",
    "# Get the index-word dictionary\n",
    "reverse_word_index = tokenizer.index_word\n",
    "\n",
    "# Get the embedding layer from the model (i.e. first layer)\n",
    "embedding_layer = model.layers[0]\n",
    "\n",
    "# Get the weights of the embedding layer\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Print the shape. Expected is (vocab_size, embedding_dim)\n",
    "print(embedding_weights.shape) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:42.891844Z",
     "start_time": "2024-04-03T16:26:42.879133Z"
    }
   },
   "id": "32213d5b394b9dd3",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Open writeable files\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# Initialize the loop. Start counting at `1` because `0` is just for the padding\n",
    "for word_num in range(1, vocab_size):\n",
    "\n",
    "  # Get the word associated at the current index\n",
    "  word_name = reverse_word_index[word_num]\n",
    "\n",
    "  # Get the embedding weights associated with the current index\n",
    "  word_embedding = embedding_weights[word_num]\n",
    "\n",
    "  # Write the word name\n",
    "  out_m.write(word_name + \"\\n\")\n",
    "\n",
    "  # Write the word embedding\n",
    "  out_v.write('\\t'.join([str(x) for x in word_embedding]) + \"\\n\")\n",
    "\n",
    "# Close the files\n",
    "out_v.close()\n",
    "out_m.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T16:26:43.077510Z",
     "start_time": "2024-04-03T16:26:42.894068Z"
    }
   },
   "id": "da7ec59c7a62ab03",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can see result in the tow .tsv files our workspace.\n",
    "C:\\Users\\User\\PycharmProjects\\Tensorfolw_Certificate_Practices\\C3_NLP\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396491e1d0bbb5be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "la    What I've leaned new in this Task : \n",
    "1/ Preprocessing data : More exactly :\n",
    "    a/ Split the dataset ;purely python syntax used on numpy array that could be really handy when preprocessing data(sentences[0:training_size] ,sentences[training_size:])\n",
    "    b/ Using num_words parameter in the tokenizer constructor this mean that for example our dictionary will be composed with only 100 different words even so the training sentences contains more than that ! In that case the longest words will be indexed and the short ones will be kicked in order to respect this  rule( I guest it's a way of avoiding over-fitting, but not sure ! we will see how it goes 🤔)\n",
    "    c/ Padding sequences : using pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    " 2/ Build and Compile the Model: \n",
    "    a/ Replacing the Flatten layer with GlobalAveragePooling1D\n",
    "    b/ Adding an Embedding yer: This layer helps to define the semantic similarity between words. By using a spatial representation, each word is represented by a vector. This facilitates categorizing words. For example, 'bad' and 'worst' will be represented by two vectors that have almost the same parameters, and the same goes for 'good' and 'awesome'.\n",
    "    c/ Visualize Word Embeddings into external tsv files."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "725c86a9c895f0d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
